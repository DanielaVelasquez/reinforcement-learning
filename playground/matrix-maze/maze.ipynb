{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import numpy as np\n",
    "from maze import get_actions\n",
    "from maze import get_possible_actions\n",
    "from maze import next_step\n",
    "from maze import get_state\n",
    "from maze import get_transition_probabilities\n",
    "from maze import get_rewards\n",
    "from maze import describe_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an agent in position `(0,0)`. The goal is get the agent to `(2,2)` using 4 possible movements: left, right, up and down. Agent will only get a reward if it gets to its goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = (2, 2)\n",
    "agent = (0, 0)\n",
    "# 0 -> Left\n",
    "# 1 -> Right\n",
    "# 2 -> Up\n",
    "# 3 -> Down\n",
    "actions = np.array([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we model this problem as a MDP, each position the agent can take in the matrix is one state. From each state, the agent can only take certain actions (i.e if the agent is at `(0,1)`, equivelant to state `1` in the maze action `U` is impossible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/imgs/MDP.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transition probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tell us what are the probabilities to get from one state to another using a specific action. Each row is a state. Each column is an action. Each element in the matrix is an array, where each position represents one state, its value if the probability of moving from the state in the row to this state, with the action in the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = get_transition_probabilities(matrix, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the agent is on state `4` (which is the center state) and it takes action `Left`, what are the probabilities for it to get to any of the other states? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_probabilities[4,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can only get to state `3` if it takes action `Left` with probability of `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Possible actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible actions an agent can take from one position. For instance on state `0` it can only either go `right` or `down`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3],\n",
       " [0, 1, 3],\n",
       " [0, 3],\n",
       " [1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 2, 3],\n",
       " [1, 2],\n",
       " [0, 1, 2],\n",
       " [0, 2]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions = get_possible_actions(matrix)\n",
    "possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe all possible actions an agent can take from a different place at the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = get_rewards(matrix, actions, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the rewards an agent can get if it is on state `4` and it moves left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[4,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing! From this position, moving left the agent will get to state `3` and that is not our goal. However what happens if I am on state `5` and I take action `down` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[5, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will get a reward of `1` if I end up on state `8`, which our goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Value Iteration algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the `Q-Value Iteration algorithm` to see what is the best action we can take from each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 3.81242949, 0.        , 3.81242949],\n",
       "       [3.42603276, 4.23603276, 0.        , 4.23603276],\n",
       "       [3.81242949, 0.        , 0.        , 4.71242949],\n",
       "       [0.        , 4.23603276, 3.42603276, 4.23603276],\n",
       "       [3.81242949, 4.71242949, 3.81242949, 4.71242949],\n",
       "       [4.23603276, 0.        , 4.23603276, 5.23603276],\n",
       "       [0.        , 4.71242949, 3.81242949, 0.        ],\n",
       "       [4.23603276, 5.23603276, 4.23603276, 0.        ],\n",
       "       [4.71242949, 0.        , 4.71242949, 0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9 # Discount factor \n",
    "\n",
    "t_states = matrix.size\n",
    "t_actions = actions.size\n",
    "Q_values = np.zeros((t_states, t_actions))\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    \n",
    "    for s in range(t_states):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s,a] = np.sum([\n",
    "                transition_probabilities[s,a,sp]\n",
    "                * (rewards[s, a, sp] + gamma * np.max(Q_prev[sp]))\n",
    "            for sp in range(t_states)])\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents a state and each column represents an action. The highest value in an action per state indicates which is the best action to take if the agent is on that state. So, what are the best actions to take on every state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 1, 1, 3, 1, 1, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_actions = np.argmax(Q_values, axis=1)\n",
    "best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Right', 'Right', 'Down', 'Right', 'Right', 'Down', 'Right', 'Right', 'Left']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[describe_action(a) for a in best_actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the optimal policy for our MDP. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
